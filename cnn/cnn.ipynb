{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4ed38d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda as cuda\n",
    "import os\n",
    "import time\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import timm # Unofficial pytorch image models, for comparison\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Use Nvidia GPU if available, for faster results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de8c4150",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define the transformations that will be applied to the images during the loading process\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m transform \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      3\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m360\u001b[39m)), \u001b[38;5;66;03m# capture resolution / 2 for now, need to experiment with different sizes\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m      5\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m]) \u001b[38;5;66;03m# Mean and Std. Dev values used here are commonly used with the ImageNet dataset\u001b[39;00m\n\u001b[1;32m      6\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the transformations that will be applied to the images during the loading process\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((640, 360)), # capture resolution / 2 for now, need to experiment with different sizes\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Mean and Std. Dev values used here are commonly used with the ImageNet dataset\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87780e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data\n",
    "# We'll use a PyTorch Generator to make things repeatable (deterministic)\n",
    "g = torch.Generator(device=device).manual_seed(2147483647)\n",
    "\n",
    "# Load Data\n",
    "dataset_path = './data'\n",
    "dataset = datasets.ImageFolder(dataset_path, transform=transform) # Automatically assigns labels to examples based on the directory structure\n",
    "\n",
    "# Generate 3 splits: Train (80%), Validation - Hyperparam selection (10%), Test (10%)\n",
    "# May need to implement k-fold cross-validation since data is relatively small in size\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "train, val, test = torch.utils.data.random_split(dataset, [train_size, val_size, test_size], generator=g)\n",
    "\n",
    "# Create Data Loaders for splits\n",
    "batch_size = 32\n",
    "train_dl = torch.utils.data.DataLoader(train, batch_size=batch_size)\n",
    "val_dl = torch.utils.data.DataLoader(val, batch_size=batch_size)\n",
    "test_dl = torch.utils.data.Dataloader(test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6978a478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Metrics\n",
    "models = [\"BasicCNN\"] #, \"VGG\", \"VisionTransformer\", \"DenseNet\", \"InceptionV4\", \"RegNet\"]\n",
    "#mtoi = {model: i for i, model in enumerate(models)}\n",
    "times = {}\n",
    "training_loss = {}\n",
    "training_accuracy = {}\n",
    "validation_loss = {}\n",
    "validation_accuracy = {}\n",
    "\n",
    "for model in models:\n",
    "    times[model] = 0\n",
    "    training_loss[model] = []\n",
    "    training_accuracy[model] = 0\n",
    "    validation_loss = []\n",
    "    validation_accuracy = 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe5a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN \n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        \n",
    "        # Define CNN layers\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size, stride)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size, stride)\n",
    "        \n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features, out_features)\n",
    "        self.relu3 = nn.Relu()\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define the forward pass through the network\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # Flatten the tensor before passing to fully connected layers\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b559f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicCNN()\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1 # Learning rate -- Implement Learning Rate Decay Later \n",
    "criterion = nn.CrossEntropyLoss() # loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # In Future, experiment with ADAM, and LION optimizers\n",
    "num_epochs = 5 \n",
    "\n",
    "loss_list = []\n",
    "\n",
    "# Training Time\n",
    "start_event = cuda.Event(enable_timing=True)\n",
    "end_event = cuda.Event(enable_timing=True)\n",
    "# Begin clock\n",
    "start_event.record()\n",
    "\n",
    "# Train BasicCNN here\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels, in train_dl:\n",
    "        optimizer.zero_grad() # Clear gradients\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss_list.append(loss) \n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "# End clock\n",
    "end_event.record()\n",
    "cuda.synchronize() # Wait for GPU operations to complete\n",
    "\n",
    "time_BasicCNN = start_event.elapsed_time(end_event) / 1000 # Convert to seconds\n",
    "global times\n",
    "times[BasicCNN] = time_BasicCNN\n",
    "\n",
    "global training_loss\n",
    "training_loss[BasicCNN].append(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3471dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train alternatives here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea8673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparisons of training statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb067585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "global models\n",
    "global training_loss\n",
    "plt.figure(figsize=(20,12))\n",
    "for model in models:\n",
    "    for loss in training_loss[model]\n",
    "        plt.plot(loss, label=f\"{model}\")\n",
    "    \n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Convergence Comparison\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e0a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Accuracy\n",
    "global models\n",
    "global training_accuracy\n",
    "\n",
    "acc_list = []\n",
    "for model in models:\n",
    "    acc_list.push(training_accuracy[model])\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "plt.bar(models, acc_list)\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ccb62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Loss\n",
    "global models\n",
    "global validation_loss\n",
    "plt.figure(figsize=(20,12))\n",
    "for model in models:\n",
    "    for loss in validation_loss[model]\n",
    "        plt.plot(loss, label=f\"{model}\")\n",
    "    \n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Validation Loss Convergence Comparison\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a6162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Accuracy\n",
    "global models\n",
    "global validation_accuracy\n",
    "\n",
    "acc_list = []\n",
    "for model in models:\n",
    "    acc_list.push(validation_accuracy[model])\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "plt.bar(models, acc_list)\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy Comparison of Different Models\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6ad551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9834629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Time\n",
    "global models\n",
    "global times\n",
    "\n",
    "times_list = []\n",
    "for model in models:\n",
    "    times_list.push(times[model])\n",
    "    \n",
    "plt.figure(figsize(20,12))\n",
    "plt.bar(models, times_list)\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Training Time\")\n",
    "plt.title(\"Training Time Comparison of Different Models\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
